{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.14","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceType":"competition","sourceId":86518,"databundleVersionId":9809560},{"sourceType":"modelInstanceVersion","sourceId":6063,"databundleVersionId":7429216,"modelInstanceId":4684},{"sourceType":"modelInstanceVersion","sourceId":6064,"databundleVersionId":7429221,"modelInstanceId":4685}],"dockerImageVersionId":30787,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import kagglehub\n\n# Download latest version\npath = kagglehub.model_download(\"keras/deberta_v3/keras/deberta_v3_extra_small_en\")\n\nprint(\"Path to model files:\", path)\n","metadata":{"execution":{"iopub.status.busy":"2024-10-31T18:22:57.021925Z","iopub.execute_input":"2024-10-31T18:22:57.022299Z","iopub.status.idle":"2024-10-31T18:22:57.392679Z","shell.execute_reply.started":"2024-10-31T18:22:57.022252Z","shell.execute_reply":"2024-10-31T18:22:57.391697Z"},"trusted":true},"execution_count":1,"outputs":[{"name":"stdout","text":"Path to model files: /kaggle/input/deberta_v3/keras/deberta_v3_extra_small_en/2\n","output_type":"stream"}]},{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2024-10-31T18:22:57.394343Z","iopub.execute_input":"2024-10-31T18:22:57.395020Z","iopub.status.idle":"2024-10-31T18:22:57.759004Z","shell.execute_reply.started":"2024-10-31T18:22:57.394966Z","shell.execute_reply":"2024-10-31T18:22:57.758148Z"},"trusted":true},"execution_count":2,"outputs":[{"name":"stdout","text":"/kaggle/input/deberta_v3/keras/deberta_v3_extra_small_en/2/config.json\n/kaggle/input/deberta_v3/keras/deberta_v3_extra_small_en/2/tokenizer.json\n/kaggle/input/deberta_v3/keras/deberta_v3_extra_small_en/2/metadata.json\n/kaggle/input/deberta_v3/keras/deberta_v3_extra_small_en/2/model.weights.h5\n/kaggle/input/deberta_v3/keras/deberta_v3_extra_small_en/2/assets/tokenizer/vocabulary.spm\n/kaggle/input/deberta_v3/keras/deberta_v3_small_en/2/config.json\n/kaggle/input/deberta_v3/keras/deberta_v3_small_en/2/tokenizer.json\n/kaggle/input/deberta_v3/keras/deberta_v3_small_en/2/metadata.json\n/kaggle/input/deberta_v3/keras/deberta_v3_small_en/2/model.weights.h5\n/kaggle/input/deberta_v3/keras/deberta_v3_small_en/2/assets/tokenizer/vocabulary.spm\n/kaggle/input/llm-classification-finetuning/sample_submission.csv\n/kaggle/input/llm-classification-finetuning/train.csv\n/kaggle/input/llm-classification-finetuning/test.csv\n","output_type":"stream"}]},{"cell_type":"code","source":"import os \nos.environ['KERAS_BACKGROUND'] = 'tensorflow'\n\nimport keras_nlp\nimport keras\nimport tensorflow as tf\nimport numpy as np\nimport pandas as pd \nfrom tqdm import tqdm\nimport matplotlib.pyplot as plt \nfrom sklearn.model_selection import StratifiedKFold\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nplt.style.use('seaborn')","metadata":{"execution":{"iopub.status.busy":"2024-10-31T20:31:44.404347Z","iopub.execute_input":"2024-10-31T20:31:44.404764Z","iopub.status.idle":"2024-10-31T20:31:44.628140Z","shell.execute_reply.started":"2024-10-31T20:31:44.404722Z","shell.execute_reply":"2024-10-31T20:31:44.627239Z"},"trusted":true},"execution_count":17,"outputs":[{"name":"stderr","text":"/tmp/ipykernel_30/3465193932.py:14: MatplotlibDeprecationWarning: The seaborn styles shipped by Matplotlib are deprecated since 3.6, as they no longer correspond to the styles shipped by seaborn. However, they will remain available as 'seaborn-v0_8-<style>'. Alternatively, directly use the seaborn API instead.\n  plt.style.use('seaborn')\n","output_type":"stream"}]},{"cell_type":"code","source":"class ModelConfig:\n    # Model architecture settings\n    backbone_type = \"deberta_v3_small_en\"\n    max_seq_length = 128  # Reduced from 256\n    training_batch = 64   # Increased from 32\n    training_cycles = 2   # Reduced from 3\n    \n    # Training parameters\n    initial_learning_rate = 1e-4  # Increased for faster convergence\n    min_learning_rate = 1e-5\n    dropout_rate = 0.1\n    attention_heads = 8\n    dense_dim = 512      # Reduced from 768\n    \n    # Performance settings\n    mixed_precision = True\n    num_folds = 2       # Reduced from 3\n    \n    target_classes = [\"winner_model_a\", \"winner_model_b\", \"winner_tie\"]\n    ","metadata":{"execution":{"iopub.status.busy":"2024-10-31T20:22:18.410841Z","iopub.execute_input":"2024-10-31T20:22:18.411613Z","iopub.status.idle":"2024-10-31T20:22:18.417423Z","shell.execute_reply.started":"2024-10-31T20:22:18.411569Z","shell.execute_reply":"2024-10-31T20:22:18.416444Z"},"trusted":true},"execution_count":14,"outputs":[]},{"cell_type":"code","source":"class DataProcessor:\n    @staticmethod\n    def load_datasets(base_path=\"/kaggle/input/llm-classification-finetuning/\"):\n        \"\"\"Load and perform initial data processing\"\"\"\n        train_data = pd.read_csv(f\"{base_path}train.csv\")\n        test_data = pd.read_csv(f\"{base_path}test.csv\")\n        return train_data, test_data\n   \n    @staticmethod\n    def process_text_pair(row):\n        \"\"\"Process prompt and response pairs with error handling\"\"\"\n        try:\n            clean_prompt = row.prompt.encode(\"utf-8\").decode(\"utf-8\")\n            clean_resp_a = row.response_a.encode(\"utf-8\").decode(\"utf-8\")\n            clean_resp_b = row.response_b.encode(\"utf-8\").decode(\"utf-8\")\n            \n            row['text_pairs'] = [\n                f\"Question: {clean_prompt}\\nAnswer: {clean_resp_a}\",\n                f\"Question: {clean_prompt}\\nAnswer: {clean_resp_b}\"\n            ]\n            row['processing_error'] = False\n        except:\n            row['text_pairs'] = [\"\", \"\"]\n            row['processing_error'] = True\n        return row\n\n# Load and process data\ntrain_df, test_df = DataProcessor.load_datasets()\ntrain_df = train_df.apply(DataProcessor.process_text_pair, axis=1)\ntest_df = test_df.apply(DataProcessor.process_text_pair, axis=1)\n\n# Remove failed processing rows\ntrain_df = train_df[~train_df['processing_error']]","metadata":{"execution":{"iopub.status.busy":"2024-10-31T18:23:10.642308Z","iopub.execute_input":"2024-10-31T18:23:10.642623Z","iopub.status.idle":"2024-10-31T18:24:17.888443Z","shell.execute_reply.started":"2024-10-31T18:23:10.642593Z","shell.execute_reply":"2024-10-31T18:24:17.887322Z"},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"code","source":"class LabelProcessor:\n    @staticmethod\n    def create_label_encoding(row):\n        \"\"\"Convert multi-column labels to single class label\"\"\"\n        if row['winner_model_b'] == 1:\n            return 1\n        elif row['winner_tie'] == 1:\n            return 2\n        return 0  # Default: model_a wins\n\nclass DataAugmenter:\n    @staticmethod\n    def swap_responses(df, swap_probability=0.5):\n        \"\"\"Augment data by swapping responses with probability\"\"\"\n        augmented = df.copy()\n        swap_mask = np.random.rand(len(df)) < swap_probability\n        \n        # Swap responses and adjust labels\n        augmented.loc[swap_mask, ['response_a', 'response_b']] = \\\n            augmented.loc[swap_mask, ['response_b', 'response_a']].values\n        \n        # Update labels for swapped entries (only for binary outcomes)\n        binary_mask = augmented['class_label'].isin([0, 1])\n        augmented.loc[swap_mask & binary_mask, 'class_label'] = \\\n            1 - augmented.loc[swap_mask & binary_mask, 'class_label']\n        \n        return pd.concat([df, augmented], ignore_index=True)\n\n# Process labels and augment data\ntrain_df['class_label'] = train_df.apply(LabelProcessor.create_label_encoding, axis=1)\ntrain_df = DataAugmenter.swap_responses(train_df)","metadata":{"execution":{"iopub.status.busy":"2024-10-31T18:24:17.889848Z","iopub.execute_input":"2024-10-31T18:24:17.890571Z","iopub.status.idle":"2024-10-31T18:24:18.772256Z","shell.execute_reply.started":"2024-10-31T18:24:17.890522Z","shell.execute_reply":"2024-10-31T18:24:18.771427Z"},"trusted":true},"execution_count":6,"outputs":[]},{"cell_type":"code","source":"class TextPreprocessor:\n    def __init__(self, config):\n        self.preprocessor = keras_nlp.models.DebertaV3Preprocessor.from_preset(\n            preset=config.backbone_type,\n            sequence_length=config.max_seq_length\n        )\n    \n    def __call__(self, text, label=None):\n        \"\"\"Process text and optionally pair with label\"\"\"\n        processed_text = self.preprocessor(text)\n        return (processed_text, label) if label is not None else processed_text\n\nclass DatasetBuilder:\n    def __init__(self, config, preprocessor):\n        self.config = config\n        self.preprocessor = preprocessor\n        \n    def build(self, texts, labels=None, shuffle=True, cache=True):\n        \"\"\"Build TensorFlow dataset with preprocessing\"\"\"\n        AUTO = tf.data.AUTOTUNE\n        \n        # Prepare data slices\n        if labels is not None:\n            labels = keras.utils.to_categorical(labels, num_classes=3)\n            slices = (texts, labels)\n        else:\n            slices = (texts,)\n            \n        # Create and configure dataset\n        dataset = tf.data.Dataset.from_tensor_slices(slices)\n        \n        # Enable parallel processing\n        options = tf.data.Options()\n        options.experimental_distribute.auto_shard_policy = \\\n            tf.data.experimental.AutoShardPolicy.DATA\n        dataset = dataset.with_options(options)\n        \n        if cache:\n            dataset = dataset.cache()\n        dataset = dataset.map(self.preprocessor, \n                            num_parallel_calls=AUTO)\n        if shuffle:\n            dataset = dataset.shuffle(buffer_size=1000)\n        return dataset.batch(self.config.training_batch).prefetch(AUTO)\n\n# Initialize preprocessor and dataset builder\npreprocessor = TextPreprocessor(ModelConfig)\ndataset_builder = DatasetBuilder(ModelConfig, preprocessor)","metadata":{"execution":{"iopub.status.busy":"2024-10-31T18:24:18.773700Z","iopub.execute_input":"2024-10-31T18:24:18.774077Z","iopub.status.idle":"2024-10-31T18:24:21.608027Z","shell.execute_reply.started":"2024-10-31T18:24:18.774034Z","shell.execute_reply":"2024-10-31T18:24:21.607107Z"},"trusted":true},"execution_count":7,"outputs":[]},{"cell_type":"code","source":"class LLMClassifier:\n    def __init__(self, config):\n        self.config = config\n        \n    def build(self):\n        # Input layers with correct naming\n        inputs = {\n            \"token_ids\": keras.Input(shape=(2, None), dtype=tf.int32, name=\"token_ids\"),\n            \"padding_mask\": keras.Input(shape=(2, None), dtype=tf.int32, name=\"padding_mask\")\n        }\n        \n        # Initialize backbone with reduced parameters\n        backbone = keras_nlp.models.DebertaV3Backbone.from_preset(\n            preset=self.config.backbone_type,\n            max_sequence_length=self.config.max_seq_length\n        )\n        \n        # Process both responses\n        response_embeddings = []\n        for i in range(2):\n            response = {k: v[:,i,:] for k,v in inputs.items()}\n            response_embeddings.append(backbone(response))\n        \n        # Simplified architecture\n        combined = keras.layers.Concatenate(axis=-1)(response_embeddings)\n        x = keras.layers.GlobalAveragePooling1D()(combined)\n        \n        # Reduced dense layers\n        x = keras.layers.Dense(self.config.dense_dim, activation=\"relu\")(x)\n        x = keras.layers.Dropout(self.config.dropout_rate)(x)\n        x = keras.layers.Dense(self.config.dense_dim // 2, activation=\"relu\")(x)\n        x = keras.layers.Dropout(self.config.dropout_rate)(x)\n        \n        outputs = keras.layers.Dense(3, activation=\"softmax\")(x)\n        return keras.Model(inputs, outputs)","metadata":{"execution":{"iopub.status.busy":"2024-10-31T18:24:21.609261Z","iopub.execute_input":"2024-10-31T18:24:21.609587Z","iopub.status.idle":"2024-10-31T18:24:21.619620Z","shell.execute_reply.started":"2024-10-31T18:24:21.609555Z","shell.execute_reply":"2024-10-31T18:24:21.618564Z"},"trusted":true},"execution_count":8,"outputs":[]},{"cell_type":"code","source":"class FocalLoss(keras.losses.Loss):\n    def __init__(self, gamma=2.0, alpha=0.25):\n        super().__init__()\n        self.gamma = gamma\n        self.alpha = alpha\n    \n    def call(self, y_true, y_pred):\n        epsilon = 1e-9\n        y_true = tf.cast(y_true, tf.float32)\n        y_pred = tf.clip_by_value(y_pred, epsilon, 1. - epsilon)\n        \n        # Calculate focal loss\n        cross_entropy = -y_true * tf.math.log(y_pred)\n        weight = tf.pow(1. - y_pred, self.gamma) * y_true\n        focal = self.alpha * weight * cross_entropy\n        \n        return tf.reduce_mean(tf.reduce_sum(focal, axis=-1))\n\nclass LearningRateScheduler:\n    @staticmethod\n    def cosine_decay_with_warmup(epoch, config):\n        \"\"\"Cosine decay schedule with warmup\"\"\"\n        cycle_length = 2\n        cycle = np.floor(1 + epoch / cycle_length)\n        x = np.abs(epoch / cycle_length - cycle)\n        return config.min_learning_rate + \\\n               (config.initial_learning_rate - config.min_learning_rate) * \\\n               max(0, (1 - x))","metadata":{"execution":{"iopub.status.busy":"2024-10-31T18:24:21.620824Z","iopub.execute_input":"2024-10-31T18:24:21.621271Z","iopub.status.idle":"2024-10-31T18:24:21.632058Z","shell.execute_reply.started":"2024-10-31T18:24:21.621224Z","shell.execute_reply":"2024-10-31T18:24:21.631088Z"},"trusted":true},"execution_count":9,"outputs":[]},{"cell_type":"code","source":"class ModelTrainer:\n    def __init__(self, config):\n        self.config = config\n        self.kfold = StratifiedKFold(\n            n_splits=config.num_folds, \n            shuffle=True, \n            random_state=42\n        )\n        \n    def train_fold(self, fold, train_data, val_data):\n        # Initialize model\n        model = LLMClassifier(self.config).build()\n        \n        # Use AMP (Automatic Mixed Precision)\n        if self.config.mixed_precision:\n            tf.keras.mixed_precision.set_global_policy(\"mixed_float16\")\n        \n        # Compile with optimized settings\n        model.compile(\n            optimizer=keras.optimizers.AdamW(\n                learning_rate=self.config.initial_learning_rate,\n                weight_decay=0.01\n            ),\n            loss=FocalLoss(alpha=0.25, gamma=2.0),\n            metrics=['accuracy'],\n            jit_compile=True  # Enable XLA compilation\n        )\n        \n        # Streamlined callbacks\n        callbacks = [\n            keras.callbacks.ModelCheckpoint(\n                f'model_fold_{fold}.weights.h5',\n                monitor='val_loss',\n                save_best_only=True,\n                save_weights_only=True,\n                verbose=0\n            ),\n            keras.callbacks.EarlyStopping(\n                monitor='val_loss',\n                patience=1,\n                restore_best_weights=True,\n                verbose=1\n            )\n        ]\n        \n        # Train with reduced verbosity\n        history = model.fit(\n            train_data,\n            epochs=self.config.training_cycles,\n            validation_data=val_data,\n            callbacks=callbacks,\n            verbose=1\n        )\n        \n        return model, history\n\n# Training execution\ntrainer = ModelTrainer(ModelConfig)\nmodels = []\n\n# Use smaller subset for training if needed\ntrain_sample = train_df.sample(frac=0.8, random_state=42)  # Use 80% of data\n\nfor fold, (train_idx, val_idx) in enumerate(trainer.kfold.split(\n    train_sample, train_sample['class_label'])):\n    print(f\"\\nFold {fold + 1}/{ModelConfig.num_folds}\")\n    \n    # Prepare fold data\n    train_fold = dataset_builder.build(\n        train_sample.iloc[train_idx]['text_pairs'].tolist(),\n        train_sample.iloc[train_idx]['class_label'].tolist()\n    )\n    val_fold = dataset_builder.build(\n        train_sample.iloc[val_idx]['text_pairs'].tolist(),\n        train_sample.iloc[val_idx]['class_label'].tolist()\n    )\n    \n    model, _ = trainer.train_fold(fold, train_fold, val_fold)\n    models.append(model)","metadata":{"execution":{"iopub.status.busy":"2024-10-31T18:24:21.634574Z","iopub.execute_input":"2024-10-31T18:24:21.634887Z","iopub.status.idle":"2024-10-31T20:17:17.296603Z","shell.execute_reply.started":"2024-10-31T18:24:21.634847Z","shell.execute_reply":"2024-10-31T20:17:17.295692Z"},"trusted":true},"execution_count":10,"outputs":[{"name":"stdout","text":"\nFold 1/2\nEpoch 1/2\n","output_type":"stream"},{"name":"stderr","text":"WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\nI0000 00:00:1730399112.349469     113 service.cc:145] XLA service 0x7cf574008440 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:\nI0000 00:00:1730399112.349538     113 service.cc:153]   StreamExecutor device (0): Tesla T4, Compute Capability 7.5\nI0000 00:00:1730399112.349544     113 service.cc:153]   StreamExecutor device (1): Tesla T4, Compute Capability 7.5\nWARNING: All log messages before absl::InitializeLog() is called are written to STDERR\nI0000 00:00:1730399181.330186     149 asm_compiler.cc:369] ptxas warning : Registers are spilled to local memory in function 'triton_gemm_dot_100', 1348 bytes spill stores, 1348 bytes spill loads\n\nI0000 00:00:1730399252.850082     113 asm_compiler.cc:369] ptxas warning : Registers are spilled to local memory in function 'loop_add_subtract_fusion_52', 40 bytes spill stores, 40 bytes spill loads\nptxas warning : Registers are spilled to local memory in function 'loop_add_subtract_fusion_48', 56 bytes spill stores, 56 bytes spill loads\n\nI0000 00:00:1730399253.004110     113 device_compiler.h:188] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n","output_type":"stream"},{"name":"stdout","text":"\u001b[1m718/719\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m2s\u001b[0m 3s/step - accuracy: 0.3561 - loss: 0.1299","output_type":"stream"},{"name":"stderr","text":"I0000 00:00:1730401131.699076     195 asm_compiler.cc:369] ptxas warning : Registers are spilled to local memory in function 'triton_gemm_dot_100', 1348 bytes spill stores, 1348 bytes spill loads\n\nI0000 00:00:1730401133.430837     193 asm_compiler.cc:369] ptxas warning : Registers are spilled to local memory in function 'triton_gemm_dot_698', 1384 bytes spill stores, 1384 bytes spill loads\n\nI0000 00:00:1730401200.372085     114 asm_compiler.cc:369] ptxas warning : Registers are spilled to local memory in function 'loop_add_subtract_fusion_45', 40 bytes spill stores, 40 bytes spill loads\nptxas warning : Registers are spilled to local memory in function 'loop_add_subtract_fusion_37', 40 bytes spill stores, 40 bytes spill loads\nptxas warning : Registers are spilled to local memory in function 'loop_add_subtract_fusion_29', 40 bytes spill stores, 40 bytes spill loads\n\n","output_type":"stream"},{"name":"stdout","text":"\u001b[1m719/719\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3s/step - accuracy: 0.3561 - loss: 0.1299","output_type":"stream"},{"name":"stderr","text":"I0000 00:00:1730401696.100388     232 asm_compiler.cc:369] ptxas warning : Registers are spilled to local memory in function 'triton_gemm_dot_134', 1384 bytes spill stores, 1384 bytes spill loads\n\n","output_type":"stream"},{"name":"stdout","text":"\u001b[1m719/719\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2630s\u001b[0m 3s/step - accuracy: 0.3561 - loss: 0.1299 - val_accuracy: 0.3853 - val_loss: 0.1204\nEpoch 2/2\n\u001b[1m719/719\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2302s\u001b[0m 3s/step - accuracy: 0.3919 - loss: 0.1203 - val_accuracy: 0.4228 - val_loss: 0.1172\nRestoring model weights from the end of the best epoch: 2.\n\nFold 2/2\nEpoch 1/2\n","output_type":"stream"},{"name":"stderr","text":"I0000 00:00:1730404192.253582     114 asm_compiler.cc:369] ptxas warning : Registers are spilled to local memory in function 'input_add_reduce_fusion_136__1', 24 bytes spill stores, 24 bytes spill loads\nptxas warning : Registers are spilled to local memory in function 'input_add_reduce_fusion_133__1', 16 bytes spill stores, 16 bytes spill loads\nptxas warning : Registers are spilled to local memory in function 'input_add_reduce_fusion_127__1', 16 bytes spill stores, 16 bytes spill loads\nptxas warning : Registers are spilled to local memory in function 'input_add_reduce_fusion_124__1', 16 bytes spill stores, 16 bytes spill loads\n\n","output_type":"stream"},{"name":"stdout","text":"\u001b[1m718/719\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 861ms/step - accuracy: 0.3510 - loss: 0.1286","output_type":"stream"},{"name":"stderr","text":"I0000 00:00:1730404874.936284     325 asm_compiler.cc:369] ptxas warning : Registers are spilled to local memory in function 'triton_gemm_dot_396', 4 bytes spill stores, 4 bytes spill loads\n\nI0000 00:00:1730404876.201577     326 asm_compiler.cc:369] ptxas warning : Registers are spilled to local memory in function 'triton_gemm_dot_404', 4 bytes spill stores, 4 bytes spill loads\n\nI0000 00:00:1730404884.888519     327 asm_compiler.cc:369] ptxas warning : Registers are spilled to local memory in function 'triton_gemm_dot_402', 4 bytes spill stores, 4 bytes spill loads\n\nI0000 00:00:1730404948.294965     112 asm_compiler.cc:369] ptxas warning : Registers are spilled to local memory in function 'input_add_reduce_fusion_130__1', 44 bytes spill stores, 44 bytes spill loads\n\n","output_type":"stream"},{"name":"stdout","text":"\u001b[1m719/719\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1s/step - accuracy: 0.3510 - loss: 0.1286   ","output_type":"stream"},{"name":"stderr","text":"I0000 00:00:1730405082.846007     419 asm_compiler.cc:369] ptxas warning : Registers are spilled to local memory in function 'triton_gemm_dot_136', 4 bytes spill stores, 4 bytes spill loads\n\nI0000 00:00:1730405086.759553     419 asm_compiler.cc:369] ptxas warning : Registers are spilled to local memory in function 'triton_gemm_dot_137', 4 bytes spill stores, 4 bytes spill loads\n\nI0000 00:00:1730405087.758750     416 asm_compiler.cc:369] ptxas warning : Registers are spilled to local memory in function 'triton_gemm_dot_132', 4 bytes spill stores, 4 bytes spill loads\n\n","output_type":"stream"},{"name":"stdout","text":"\u001b[1m719/719\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1081s\u001b[0m 1s/step - accuracy: 0.3510 - loss: 0.1286 - val_accuracy: 0.3767 - val_loss: 0.1214\nEpoch 2/2\n\u001b[1m719/719\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m739s\u001b[0m 1s/step - accuracy: 0.3759 - loss: 0.1214 - val_accuracy: 0.3827 - val_loss: 0.1221\nEpoch 2: early stopping\nRestoring model weights from the end of the best epoch: 1.\n","output_type":"stream"}]},{"cell_type":"code","source":"class Predictor:\n    def __init__(self, config):\n        self.config = config\n        self.target_classes = config.target_classes\n        \n    def analyze_confidence_levels(self, predictions, submission):\n        \"\"\"Analyze predictions at different confidence thresholds\"\"\"\n        confidence_thresholds = [0.33, 0.4, 0.45, 0.5]\n        \n        print(\"\\n=== Confidence Analysis ===\")\n        print(\"\\nConfidence Distribution:\")\n        \n        # Analyze different confidence thresholds\n        for threshold in confidence_thresholds:\n            print(f\"\\nPredictions with confidence > {threshold:.2f}:\")\n            for cls in self.target_classes:\n                confident_preds = (submission[cls] > threshold).sum()\n                percentage = confident_preds/len(submission)*100\n                print(f\"{cls}: {confident_preds} predictions ({percentage:.2f}%)\")\n        \n        # Find maximum prediction for each sample\n        max_confidences = np.max(predictions, axis=1)\n        \n        print(\"\\nConfidence Statistics:\")\n        print(f\"Mean confidence: {max_confidences.mean():.3f}\")\n        print(f\"Median confidence: {np.median(max_confidences):.3f}\")\n        print(f\"Max confidence: {max_confidences.max():.3f}\")\n        print(f\"Min confidence: {max_confidences.min():.3f}\")\n        \n        # Analyze class-wise predictions\n        print(\"\\nClass-wise Maximum Probabilities:\")\n        for i, cls in enumerate(self.target_classes):\n            class_max = np.max(predictions[:, i])\n            class_mean = np.mean(predictions[:, i])\n            print(f\"{cls}:\")\n            print(f\"  Max probability: {class_max:.3f}\")\n            print(f\"  Mean probability: {class_mean:.3f}\")\n    \n    def plot_training_evaluation(self, predictions, submission):\n        \"\"\"Enhanced visualization of model predictions\"\"\"\n        plt.style.use('seaborn')\n        fig = plt.figure(figsize=(20, 15))\n        \n        # 1. Enhanced Class Distribution Plot\n        plt.subplot(3, 2, 1)\n        class_means = [submission[cls].mean() for cls in self.target_classes]\n        bars = plt.bar(self.target_classes, class_means)\n        plt.title('Mean Prediction Distribution', fontsize=12)\n        plt.xticks(rotation=45)\n        plt.ylabel('Mean Probability')\n        \n        # Add value labels on bars\n        for bar in bars:\n            height = bar.get_height()\n            plt.text(bar.get_x() + bar.get_width()/2., height,\n                    f'{height:.3f}',\n                    ha='center', va='bottom')\n        \n        # 2. Prediction Density Plot\n        plt.subplot(3, 2, 2)\n        for cls in self.target_classes:\n            sns.kdeplot(data=submission[cls], label=cls)\n        plt.title('Prediction Density Distribution', fontsize=12)\n        plt.xlabel('Prediction Value')\n        plt.ylabel('Density')\n        plt.legend()\n        \n        # 3. Correlation Heatmap\n        plt.subplot(3, 2, 3)\n        correlation = submission[self.target_classes].corr()\n        sns.heatmap(correlation, annot=True, cmap='coolwarm', fmt='.2f')\n        plt.title('Prediction Correlation Matrix', fontsize=12)\n        \n        # 4. Confidence Distribution\n        plt.subplot(3, 2, 4)\n        max_probs = np.max(predictions, axis=1)\n        plt.hist(max_probs, bins=50, edgecolor='black')\n        plt.axvline(x=0.33, color='r', linestyle='--', label='33% threshold')\n        plt.axvline(x=0.5, color='g', linestyle='--', label='50% threshold')\n        plt.title('Model Confidence Distribution', fontsize=12)\n        plt.xlabel('Maximum Prediction Probability')\n        plt.ylabel('Count')\n        plt.legend()\n        \n        # 5. Prediction Scatter Plot\n        plt.subplot(3, 2, 5)\n        plt.scatter(range(len(predictions)), max_probs, alpha=0.5)\n        plt.axhline(y=0.33, color='r', linestyle='--', label='33% threshold')\n        plt.axhline(y=0.5, color='g', linestyle='--', label='50% threshold')\n        plt.title('Prediction Confidence by Sample', fontsize=12)\n        plt.xlabel('Sample Index')\n        plt.ylabel('Maximum Prediction Probability')\n        plt.legend()\n        \n        # 6. Class-wise Box Plot\n        plt.subplot(3, 2, 6)\n        sns.boxplot(data=submission[self.target_classes])\n        plt.title('Class-wise Prediction Distribution', fontsize=12)\n        plt.xticks(rotation=45)\n        \n        plt.tight_layout()\n        plt.savefig('model_evaluation.png', dpi=300, bbox_inches='tight')\n        plt.close()\n        \n        # Print detailed analysis\n        self.analyze_confidence_levels(predictions, submission)\n\n    # ... rest of the Predictor class remains the same ...\n    \n    def print_evaluation_metrics(self, submission):\n        \"\"\"Print detailed evaluation metrics\"\"\"\n        print(\"\\n=== Detailed Evaluation Metrics ===\")\n        \n        # Basic statistics\n        print(\"\\nClass-wise Statistics:\")\n        stats = submission[self.target_classes].describe()\n        print(stats)\n        \n        # Confidence metrics\n        print(\"\\nConfidence Metrics:\")\n        for cls in self.target_classes:\n            confident_preds = (submission[cls] > 0.5).sum()\n            print(f\"{cls}: {confident_preds} confident predictions \"\n                  f\"({confident_preds/len(submission)*100:.2f}%)\")\n    \n    def predict_single_model(self, model, test_data):\n        try:\n            predictions = model.predict(\n                test_data,\n                batch_size=self.config.training_batch,\n                verbose=1\n            )\n            return predictions\n        except Exception as e:\n            print(f\"Error in model prediction: {str(e)}\")\n            return None\n    \n    def ensemble_predict(self, models, test_data):\n        print(f\"\\nGenerating predictions using {len(models)} models...\")\n        \n        all_predictions = []\n        \n        for i, model in enumerate(models, 1):\n            print(f\"\\nPredicting with model {i}/{len(models)}\")\n            model_preds = self.predict_single_model(model, test_data)\n            \n            if model_preds is not None:\n                all_predictions.append(model_preds)\n        \n        if not all_predictions:\n            raise ValueError(\"No valid predictions were generated!\")\n        \n        final_predictions = np.mean(all_predictions, axis=0)\n        return final_predictions\n\n# Usage code\ntry:\n    config = ModelConfig()\n    predictor = Predictor(config)\n    \n    print(\"Generating predictions...\")\n    predictions = predictor.ensemble_predict(models, test_dataset)\n    \n    # Normalize predictions if needed\n    predictions = predictions / predictions.sum(axis=1, keepdims=True)\n    \n    submission = pd.DataFrame({\n        'id': test_df['id'],\n        **{class_name: predictions[:, i] \n           for i, class_name in enumerate(config.target_classes)}\n    })\n    \n    # Generate enhanced evaluation plots and metrics\n    predictor.plot_training_evaluation(predictions, submission)\n    \n    # Save submission with normalized predictions\n    submission.to_csv('submission.csv', index=False)\n    print(\"\\nSubmission file created successfully!\")\n    \nexcept Exception as e:\n    print(f\"\\nError in prediction process: {str(e)}\")\n    raise\n\nfinally:\n    print(\"\\nPrediction process completed.\")","metadata":{"execution":{"iopub.status.busy":"2024-10-31T20:34:40.313341Z","iopub.execute_input":"2024-10-31T20:34:40.313768Z","iopub.status.idle":"2024-10-31T20:34:45.047217Z","shell.execute_reply.started":"2024-10-31T20:34:40.313729Z","shell.execute_reply":"2024-10-31T20:34:45.046225Z"},"trusted":true},"execution_count":19,"outputs":[{"name":"stdout","text":"Generating predictions...\n\nGenerating predictions using 2 models...\n\nPredicting with model 1/2\n\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 101ms/step\n\nPredicting with model 2/2\n\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 97ms/step\n","output_type":"stream"},{"name":"stderr","text":"/tmp/ipykernel_30/3880226114.py:41: MatplotlibDeprecationWarning: The seaborn styles shipped by Matplotlib are deprecated since 3.6, as they no longer correspond to the styles shipped by seaborn. However, they will remain available as 'seaborn-v0_8-<style>'. Alternatively, directly use the seaborn API instead.\n  plt.style.use('seaborn')\n/opt/conda/lib/python3.10/site-packages/seaborn/_oldcore.py:1119: FutureWarning: use_inf_as_na option is deprecated and will be removed in a future version. Convert inf values to NaN before operating instead.\n  with pd.option_context('mode.use_inf_as_na', True):\n/opt/conda/lib/python3.10/site-packages/seaborn/_oldcore.py:1119: FutureWarning: use_inf_as_na option is deprecated and will be removed in a future version. Convert inf values to NaN before operating instead.\n  with pd.option_context('mode.use_inf_as_na', True):\n/opt/conda/lib/python3.10/site-packages/seaborn/_oldcore.py:1119: FutureWarning: use_inf_as_na option is deprecated and will be removed in a future version. Convert inf values to NaN before operating instead.\n  with pd.option_context('mode.use_inf_as_na', True):\n","output_type":"stream"},{"name":"stdout","text":"\n=== Confidence Analysis ===\n\nConfidence Distribution:\n\nPredictions with confidence > 0.33:\nwinner_model_a: 0 predictions (0.00%)\nwinner_model_b: 2 predictions (66.67%)\nwinner_tie: 2 predictions (66.67%)\n\nPredictions with confidence > 0.40:\nwinner_model_a: 0 predictions (0.00%)\nwinner_model_b: 0 predictions (0.00%)\nwinner_tie: 1 predictions (33.33%)\n\nPredictions with confidence > 0.45:\nwinner_model_a: 0 predictions (0.00%)\nwinner_model_b: 0 predictions (0.00%)\nwinner_tie: 1 predictions (33.33%)\n\nPredictions with confidence > 0.50:\nwinner_model_a: 0 predictions (0.00%)\nwinner_model_b: 0 predictions (0.00%)\nwinner_tie: 0 predictions (0.00%)\n\nConfidence Statistics:\nMean confidence: 0.393\nMedian confidence: 0.371\nMax confidence: 0.457\nMin confidence: 0.352\n\nClass-wise Maximum Probabilities:\nwinner_model_a:\n  Max probability: 0.329\n  Mean probability: 0.311\nwinner_model_b:\n  Max probability: 0.371\n  Mean probability: 0.326\nwinner_tie:\n  Max probability: 0.457\n  Mean probability: 0.363\n\nSubmission file created successfully!\n\nPrediction process completed.\n","output_type":"stream"}]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}